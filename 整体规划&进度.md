# LLM-based 软件软修
3.30
总结一下目前大家看论文的进度，把大概用的到的信息整理出来
### AdaPatcher
**prompt表示方法**
+ 假设有一个有错误的代码文件 c 和一个已修正的代码文件 y，生成diff文件
+ 代码一致性指标：r/k;，k表示固定代码中的代码行总数，以及r表示修改后代码中保留的代码行数。（**可作为一个指标用来改善LLM的修复能力**）

### DEVLoRe
**有用的函数**
+ MethodRecorder：记录运行失败测试时调用的所有方法签名（但是还是需要我们拥有测试样例和test代码）
+ DebugRecorder 获取定位方法内的详细**调试信息**(同上需要我们拥有测试样例和test代码)

**prompt表示方法**
+ 也是使用diff格式的补丁
+ 固定模版：General Task Prompt + Input Prompt + Expected Output Prompt
    + Input Prompt: {related_methods} {error_stack} {issue_content} {debugging_info}
    + Expected Output Prompt: Please localize/generate......


### FuseFL
**定位技术**
+ SBFL：利用coverage进行覆盖率分析，然后根据代码的“可疑度”排序选择最可疑的线路作为故障代码

**数据库/代码描述**
+ 模版
![](FuseFL/image.png)
错误代码（Faulty Code）：[ ]
任务描述（Task Description）：[ ]
测试结果（Test Results）：该代码产生了错误的结果。例如：
以下列方式运行函数 [输入] 时，在第 [i] 行发生了 [错误名称]：[代码内容]；
以下列方式运行函数 [输入]，输出为 [输出结果]，但期望的结果是 [期望输出]。
基于频谱的故障定位（SBFL）技术结果：我们使用 SBFL 技术来识别潜在的故障代码行。以下是根据得分降序排序的前 [X] 条可疑代码行结果：
第 [i] 行：[代码内容]，[技术名称] 得分：[得分]
第 [j] 行：[代码内容]，[技术名称] 得分：[得分]
请分析提供的代码、测试结果以及 SBFL 技术的结果，以帮助识别潜在的故障代码行。
请使用以下模板，以 JSON 格式提供结果：
{
  "faultLoc": [
    {
      "faultyLine": [可疑代码的行号],
      "code": [该行实际代码],
      "explanation": [关于为什么该位置被认为是潜在错误的逐步推理说明]
    },
    ...
  ]
}

+ 代码描述库：https://github.com/githubhuyang/refactory


### ChatRepair
**迭代修复** ，可以考虑，但是先把非迭代的做好吧

**修复场景**
+ 单行修复（优先级最高）
+ 单块修复（优先级第二）
+ 单函数修复（优先级第三）

**prompt表示方法**
+ 测试名称
+ 导致失败的相关代码行
+ 产生的错误信息
+ 正确情况下的预期输出和函数行为信息

### Toggle
* Prompt 1：完整函数替换（不使用任何位置偏置信息）
  + 描述：给出完整的 buggy 函数，要求模型生成完整的修复函数。
* Prompt 2：提供 shared prefix，预测 truncated 修复函数
  + 描述：在输入中提供共享前缀（如函数定义等），buggy 函数中保留前缀部分，模型只需生成从 bug 起始处的修复代码。
* Prompt 3：去掉 shared prefix，引导模型从 bug 开始生成修复代码
  + 描述：输入中省略前缀，buggy 函数从 bug 开始；模型预测修复代码后再拼接前缀得到完整函数。
* Prompt 4：去掉 shared prefix 和 suffix，模型仅需预测 bug 的替换部分
  + 描述：输入中不包含前缀和后缀，只包含 buggy 部分；修复时将替换段生成后再拼接前后缀。  


---
现在最重要的是探究可行性，相信大家看完了论文之后，大概知道了如何结合LLM去进行软件修复。
但是我们看到的基本都是集成好的一整套修复逻辑，而我们现在需要自己搞出一套会比较困难，直接沿用他们的就容易导致重复的工作，老师那边未必过得了关。
所以不妨先简陋些，至少我们需要一套能够自己跑起来的流程，然后我们再慢慢去完善。
我想了想，大概分以下几个阶段
1. 首先要确定好一个**prompt输入方式**，最好是可修改的（因为可能要进行横向对比/消融实验，探究不同组合下哪种prompt最好），如果将已有的buggy数据库转化为prompt输入是我们目前要攻克的第一个大问题，这一步我们**可以借用论文中的方法**（我们应该优先选不需要测试样例/test代码的emm，因为如果这样我们的选择就只能限制在某个数据库了；不过也都试一试吧）。
>这里需要大家去真实地看一下他们是如何生成prompt的，最好能自己动手模仿他们生成一个自己写的代码的，看看工作量如何（如果工作量不大，我们就可以有自己的buggy代码了）。
2. 确定好prompt输入方式后，需要探究如何**把prompt给到大模型**，这一步也可以进行横向的探究，比如说分步给，迭代着给，一次性给，这一阶段时间充足的话可以写一个接口方便与大模型对接。
3. 当我们把错误信息给大模型之后，大模型会返回给我们一个修复方案，我们要思考**如何去评价/验证（如果可以的话）这个修复方案**，报告的结果应该也就出自于这里了。
4. 整合信息
---
大家的任务till——4.1周二下课
完成第一步，各自看看自己负责部分的prompt生成，课后讨论一下确定**prompt输入方式**即可。
（不用太急hhh，完不成也没关系，我们还有时间）



---
4.2至4.5周六晚上
最主要的任务：开题报告，确定内容
开题报告（3分钟）：组队、题目、内容、计划
+ 不知道要不要做ppt？（我上课没听，不知道老师说了没用），不过做一个应该也不用很久，我来做一下吧
+ 需要**确定好我们的内容和计划**，现有的想法就是对比不同的prompt输入方式，可能内容不够充实，所以还需要**想一些比较创新且有一定工作量的点**

我提一些可能的方向：
1. 用一些领域知识对大模型进行预训练是否会有明显成效？（比如先给它与该项目有关的几篇论文之类的）
2. 在评价体系中，加入对代码风格的评估，目的是为了让代码风格尽量保持不变
3. 现实情况中，往往一次迭代下不能彻底修复bug，所以需要**多轮迭代**，
4. 加入“自我反思机制”，每次LLM生成反馈的时候让它反思一下，是否存在问题，如果存在问题，是否需要进行修改。即它自己的“自我迭代”

**欢迎大家思考并且写下来**

关于prompt生成，目前保底估计就是用defects4j数据库，从库到prompt的脚本我已经写好了一版，ChatRepair的函数也基本上可以用来直接生成最终的prompt。我可以整合一下

这段时间我去调研一下怎么使用世航的那两个java函数，然后世航可以写一版你那里的生成prompt的脚本，康悦那边丢掉自适应什么的内容，没有源码的话去看看FuseFL里的SBFL（计算故障代码中每一行的可疑分数），因为Defects4j里也有测试样例，如果这两个可以接起来的话，把SBFL的可疑性结果作为prompt的一部分很有可能会提高修复效果，这个应该是可以算比较创新的点（）

[开题报告](./软测导开题报告.pdf软)
---
4.7开题报告结束
**反馈**
>第二阶段做消融步骤的时候，不能想当然地把不同信息消融掉，因为即使我们固定了模版，大模型本身处理的时候也会收到prompt内信息输入顺序的先后影响。这样一来，可能就不那么有说服力。或者说，有过多的因素会影响大模型的能力，我们的任务是需要研究一下这里面的影响因素并且确保我们的消融实验是有说服力的

个人意见：一开始其实我觉得老师说的，输入顺序的问题不是一个很大的问题x但是稍微搜索了一下看来确实是有这个可能，所以我们不妨先看看论文趴。

一些论文：
[输入顺序偏差对软件故障定位大型语言模型的影响](https://arxiv.org/abs/2412.18750?utm_source=chatgpt.com)

[前提顺序在大型语言模型推理中很重要](https://arxiv.org/abs/2402.08939?utm_source=chatgpt.com)

[The Order Effect: Investigating Prompt Sensitivity in Closed-Source LLMs](https://www.researchgate.net/publication/388791531_The_Order_Effect_Investigating_Prompt_Sensitivity_in_Closed-Source_LLMs?utm_source=chatgpt.com)

[顺序问题：探索多模态大型语言模型中的顺序敏感性](https://arxiv.org/html/2410.16983v1?utm_source=chatgpt.com)

[Lost in the Middle: How Language Models Use Long Contexts](https://direct.mit.edu/tacl/article/doi/10.1162/tacl_a_00638/119630/Lost-in-the-Middle-How-Language-Models-Use-Long)

[Premise Order Matters in Reasoning with Large Language Models](https://arxiv.org/html/2402.08939v1)


询问大模型后的意见
将实验拆分为两个层级：
+ 第一层：固定信息顺序的前提下，研究不同信息类型的组合影响（如：仅保留错误类型/堆栈跟踪/代码上下文等）
+ 第二层：在最优信息组合下，研究信息排列顺序的影响（如：错误描述前置 vs 代码上下文前置）

然后第二层我觉得确实可以做点文章（？）探究最佳的模版之后，我们可以模仿现有的论文，然后类似地分析一下产生这样结果的原因

第一层就还是我们原本的计划，探究不同信息组合下，信息输入顺序的影响
---
4.20-4.29开题报告
目标：
1. 确定好我们的prompt里要放什么元素
2. 确定提取这些元素的方式（有一些东西目前数据库暂时不提供，可能需要我们自己跑，或者用其他工具）
3. 完善生成prompt的脚本，确保我们需要的元素都在上面。可以在以下`确定元素`章节确定完各自负责的任务写好各自的脚本后合在一起。
4. **最好**可以在脚本中加入一个`ruffle函数`，可以随机打乱prompt内元素的顺序，然后看看效果如何
5. 最重要的是要能**跑通代码了**，即必须要把做好的一般输入给大模型了

### 确定元素
例子：假设使用指令defects4j info -p Lang -b 1可以获得到一堆东西。
1. 源错误代码：哪些测试类和测试方法触发该bug
   + bug相关的类，defects4j export -p classes.modified
   + 触发bug的测试：defects4j export -p tests.trigger
   + 源代码：./src/main和./src/test
   + **断言**：即规范对应的变量的语句
2. 函数注释：我看了一下，defects4j里函数的注释还是不少的
3. 错误输出与正确输出
   + 步骤
     + defects4j checkout -p Lang -v 1b -w Lang_buggy ，提取buggy代码和相关信息
     + cd Lang_buggy + defects4j test 得到错误输出
     + defects4j checkout -p Lang -v 1f -w Lang_fixed ， 提取修复后的代码
     + cd Lang_fixed + defects4j test 得到正确输出
3. 数据流信息：LLMDFA: Analyzing Dataflow in Code with Large Language Models[https://arxiv.org/abs/2402.10754]
   + 步骤
     + 用defects4j提取缺陷样本， defects4j checkout -p Lang -v 1b -w Lang_buggy
     + 确定source和sink（这个是LLMDFA）里的
     + 实现LLMDFA分析

我负责第一二三点，世航负责第四点。

以及我现在还在搞一个[JBMC](https://github.com/diffblue/cbmc/tree/develop/jbmc),静态java代码分析工具，能够不依赖“测试样例”的情况下输出错误路径包括导致错误的输入值、关键分支决策和变量状态...尝试搞出点东西来（）



