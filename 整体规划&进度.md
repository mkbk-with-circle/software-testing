# LLM-based 软件软修
3.30
总结一下目前大家看论文的进度，把大概用的到的信息整理出来
### AdaPatcher
**prompt表示方法**
+ 假设有一个有错误的代码文件 c 和一个已修正的代码文件 y，生成diff文件
+ 代码一致性指标：r/k;，k表示固定代码中的代码行总数，以及r表示修改后代码中保留的代码行数。（**可作为一个指标用来改善LLM的修复能力**）

### DEVLoRe
**有用的函数**
+ MethodRecorder：记录运行失败测试时调用的所有方法签名（但是还是需要我们拥有测试样例和test代码）
+ DebugRecorder 获取定位方法内的详细**调试信息**(同上需要我们拥有测试样例和test代码)

**prompt表示方法**
+ 也是使用diff格式的补丁
+ 固定模版：General Task Prompt + Input Prompt + Expected Output Prompt
    + Input Prompt: {related_methods} {error_stack} {issue_content} {debugging_info}
    + Expected Output Prompt: Please localize/generate......


### FuseFL
**定位技术**
+ SBFL：利用coverage进行覆盖率分析，然后根据代码的“可疑度”排序选择最可疑的线路作为故障代码

**数据库/代码描述**
+ 模版
![](FuseFL/image.png)
错误代码（Faulty Code）：[ ]
任务描述（Task Description）：[ ]
测试结果（Test Results）：该代码产生了错误的结果。例如：
以下列方式运行函数 [输入] 时，在第 [i] 行发生了 [错误名称]：[代码内容]；
以下列方式运行函数 [输入]，输出为 [输出结果]，但期望的结果是 [期望输出]。
基于频谱的故障定位（SBFL）技术结果：我们使用 SBFL 技术来识别潜在的故障代码行。以下是根据得分降序排序的前 [X] 条可疑代码行结果：
第 [i] 行：[代码内容]，[技术名称] 得分：[得分]
第 [j] 行：[代码内容]，[技术名称] 得分：[得分]
请分析提供的代码、测试结果以及 SBFL 技术的结果，以帮助识别潜在的故障代码行。
请使用以下模板，以 JSON 格式提供结果：
{
  "faultLoc": [
    {
      "faultyLine": [可疑代码的行号],
      "code": [该行实际代码],
      "explanation": [关于为什么该位置被认为是潜在错误的逐步推理说明]
    },
    ...
  ]
}

+ 代码描述库：https://github.com/githubhuyang/refactory


### ChatRepair
**迭代修复** ，可以考虑，但是先把非迭代的做好吧

**修复场景**
+ 单行修复（优先级最高）
+ 单块修复（优先级第二）
+ 单函数修复（优先级第三）

**prompt表示方法**
+ 测试名称
+ 导致失败的相关代码行
+ 产生的错误信息
+ 正确情况下的预期输出和函数行为信息

### Toggle
* Prompt 1：完整函数替换（不使用任何位置偏置信息）
  + 描述：给出完整的 buggy 函数，要求模型生成完整的修复函数。
* Prompt 2：提供 shared prefix，预测 truncated 修复函数
  + 描述：在输入中提供共享前缀（如函数定义等），buggy 函数中保留前缀部分，模型只需生成从 bug 起始处的修复代码。
* Prompt 3：去掉 shared prefix，引导模型从 bug 开始生成修复代码
  + 描述：输入中省略前缀，buggy 函数从 bug 开始；模型预测修复代码后再拼接前缀得到完整函数。
* Prompt 4：去掉 shared prefix 和 suffix，模型仅需预测 bug 的替换部分
  + 描述：输入中不包含前缀和后缀，只包含 buggy 部分；修复时将替换段生成后再拼接前后缀。  


---
现在最重要的是探究可行性，相信大家看完了论文之后，大概知道了如何结合LLM去进行软件修复。
但是我们看到的基本都是集成好的一整套修复逻辑，而我们现在需要自己搞出一套会比较困难，直接沿用他们的就容易导致重复的工作，老师那边未必过得了关。
所以不妨先简陋些，至少我们需要一套能够自己跑起来的流程，然后我们再慢慢去完善。
我想了想，大概分以下几个阶段
1. 首先要确定好一个**prompt输入方式**，最好是可修改的（因为可能要进行横向对比/消融实验，探究不同组合下哪种prompt最好），如果将已有的buggy数据库转化为prompt输入是我们目前要攻克的第一个大问题，这一步我们**可以借用论文中的方法**（我们应该优先选不需要测试样例/test代码的emm，因为如果这样我们的选择就只能限制在某个数据库了；不过也都试一试吧）。
>这里需要大家去真实地看一下他们是如何生成prompt的，最好能自己动手模仿他们生成一个自己写的代码的，看看工作量如何（如果工作量不大，我们就可以有自己的buggy代码了）。
2. 确定好prompt输入方式后，需要探究如何**把prompt给到大模型**，这一步也可以进行横向的探究，比如说分步给，迭代着给，一次性给，这一阶段时间充足的话可以写一个接口方便与大模型对接。
3. 当我们把错误信息给大模型之后，大模型会返回给我们一个修复方案，我们要思考**如何去评价/验证（如果可以的话）这个修复方案**，报告的结果应该也就出自于这里了。
4. 整合信息
---
大家的任务till——4.1周二下课
完成第一步，各自看看自己负责部分的prompt生成，课后讨论一下确定**prompt输入方式**即可。
（不用太急hhh，完不成也没关系，我们还有时间）



---
4.2至4.5周六晚上
最主要的任务：开题报告，确定内容
开题报告（3分钟）：组队、题目、内容、计划
+ 不知道要不要做ppt？（我上课没听，不知道老师说了没用），不过做一个应该也不用很久，我来做一下吧
+ 需要**确定好我们的内容和计划**，现有的想法就是对比不同的prompt输入方式，可能内容不够充实，所以还需要**想一些比较创新且有一定工作量的点**

我提一些可能的方向：
1. 用一些领域知识对大模型进行预训练是否会有明显成效？（比如先给它与该项目有关的几篇论文之类的）
2. 在评价体系中，加入对代码风格的评估，目的是为了让代码风格尽量保持不变
3. 现实情况中，往往一次迭代下不能彻底修复bug，所以需要**多轮迭代**，
4. 加入“自我反思机制”，每次LLM生成反馈的时候让它反思一下，是否存在问题，如果存在问题，是否需要进行修改。即它自己的“自我迭代”

**欢迎大家思考并且写下来**

关于prompt生成，目前保底估计就是用defects4j数据库，从库到prompt的脚本我已经写好了一版，ChatRepair的函数也基本上可以用来直接生成最终的prompt。我可以整合一下

这段时间我去调研一下怎么使用世航的那两个java函数，然后世航可以写一版你那里的生成prompt的脚本，康悦那边丢掉自适应什么的内容，没有源码的话去看看FuseFL里的SBFL（计算故障代码中每一行的可疑分数），因为Defects4j里也有测试样例，如果这两个可以接起来的话，把SBFL的可疑性结果作为prompt的一部分很有可能会提高修复效果，这个应该是可以算比较创新的点（）

[开题报告](./软测导开题报告.pdf软)
---
4.7开题报告结束
**反馈**
>第二阶段做消融步骤的时候，不能想当然地把不同信息消融掉，因为即使我们固定了模版，大模型本身处理的时候也会收到prompt内信息输入顺序的先后影响。这样一来，可能就不那么有说服力。或者说，有过多的因素会影响大模型的能力，我们的任务是需要研究一下这里面的影响因素并且确保我们的消融实验是有说服力的

个人意见：一开始其实我觉得老师说的，输入顺序的问题不是一个很大的问题x但是稍微搜索了一下看来确实是有这个可能，所以我们不妨先看看论文趴。

一些论文：
[输入顺序偏差对软件故障定位大型语言模型的影响](https://arxiv.org/abs/2412.18750?utm_source=chatgpt.com)

[前提顺序在大型语言模型推理中很重要](https://arxiv.org/abs/2402.08939?utm_source=chatgpt.com)

[The Order Effect: Investigating Prompt Sensitivity in Closed-Source LLMs](https://www.researchgate.net/publication/388791531_The_Order_Effect_Investigating_Prompt_Sensitivity_in_Closed-Source_LLMs?utm_source=chatgpt.com)

[顺序问题：探索多模态大型语言模型中的顺序敏感性](https://arxiv.org/html/2410.16983v1?utm_source=chatgpt.com)

[Lost in the Middle: How Language Models Use Long Contexts](https://direct.mit.edu/tacl/article/doi/10.1162/tacl_a_00638/119630/Lost-in-the-Middle-How-Language-Models-Use-Long)

[Premise Order Matters in Reasoning with Large Language Models](https://arxiv.org/html/2402.08939v1)


询问大模型后的意见
将实验拆分为两个层级：
+ 第一层：固定信息顺序的前提下，研究不同信息类型的组合影响（如：仅保留错误类型/堆栈跟踪/代码上下文等）
+ 第二层：在最优信息组合下，研究信息排列顺序的影响（如：错误描述前置 vs 代码上下文前置）

然后第二层我觉得确实可以做点文章（？）探究最佳的模版之后，我们可以模仿现有的论文，然后类似地分析一下产生这样结果的原因

第一层就还是我们原本的计划，探究不同信息组合下，信息输入顺序的影响
---
4.20-4.29开题报告
目标：
1. 确定好我们的prompt里要放什么元素
2. 确定提取这些元素的方式（有一些东西目前数据库暂时不提供，可能需要我们自己跑，或者用其他工具）
3. 完善生成prompt的脚本，确保我们需要的元素都在上面。可以在以下`确定元素`章节确定完各自负责的任务写好各自的脚本后合在一起。
4. **最好**可以在脚本中加入一个`ruffle函数`，可以随机打乱prompt内元素的顺序，然后看看效果如何
5. 最重要的是要能**跑通代码了**，即必须要把做好的一般输入给大模型了

### 确定元素
例子：假设使用指令defects4j info -p Lang -b 1可以获得到一堆东西。
1. 源错误代码：哪些测试类和测试方法触发该bug
   + bug相关的类，defects4j export -p classes.modified
   + 触发bug的测试：defects4j export -p tests.trigger
   + 源代码：./src/main和./src/test
   + **断言**：即规范对应的变量的语句
2. 函数注释：我看了一下，defects4j里函数的注释还是不少的
3. 错误输出与正确输出
   + 步骤
     + defects4j checkout -p Lang -v 1b -w Lang_buggy ，提取buggy代码和相关信息
     + cd Lang_buggy + defects4j test 得到错误输出
     + defects4j checkout -p Lang -v 1f -w Lang_fixed ， 提取修复后的代码
     + cd Lang_fixed + defects4j test 得到正确输出
3. 数据流信息：LLMDFA: Analyzing Dataflow in Code with Large Language Models[https://arxiv.org/abs/2402.10754]
   + 步骤
     + 用defects4j提取缺陷样本， defects4j checkout -p Lang -v 1b -w Lang_buggy
     + 确定source和sink（这个是LLMDFA）里的
     + 实现LLMDFA分析

我负责第一二三点，世航负责第四点。

以及我现在还在搞一个[JBMC](https://github.com/diffblue/cbmc/tree/develop/jbmc),静态java代码分析工具，能够不依赖“测试样例”的情况下输出错误路径包括导致错误的输入值、关键分支决策和变量状态...尝试搞出点东西来（）

--- 
4.20-4.24
已完成：
1. 前三点的提取脚本已经写完，主要利用chatrepair来提取信息然后整合成prompt脚本，可看[示例](./元铭宇/my_code/demo_prompt.md).具体过程请看[个人进度](./元铭宇/进度.md)
2. 另外写了一个脚本，因为上一步中利用chatrepair提取信息需要用到defects4j里项目的patch，但是它本身没有给全，所以把目前这个项目（Lang）的patch都生成了一遍，对比后与原本的没有差异。

如果大家想要运行我的代码，要先配置好[chatrepair](https://github.com/Aric3/an-implementation-of-chatrepair),以及我的建议是用docker开个镜像：运行
```bash
docker pull youruser/defects4j-env:latest
```
![](/元铭宇/pic/5.png)然后在里面跑
同样的也要配置好[defects4j](https://github.com/rjust/defects4j)

具体的命令分别是：（记得修改文件的路径）
```sh
python3 get_all_patch.py Lang #获取所有的patch
python3 extract_from_chatrepair.py #获取信息并且输出
```
有哪里不懂的直接问我就行


感觉信息还是有点少，再找找吧...不过至少有信息可以输入给大模型了。总之周末一定要开始动手和大模型交互了！哦还有，我打算借鉴chatrepair的prompt格式，开头先加一个example输入，和一个example回答（）不知道会不会好一点（至少显得充实一点hh）


---
中期汇报-5.7
世航继续去研究**gitbug-java**

我看了一下[bugs-dot-jar/bugs.jar](https://github.com/bugs-dot-jar/bugs-dot-jar)，里面的bug数据都太老了，有的甚至是十几年前的东西，放到现在的java环境中很可能被修复了所以打算抛弃
另外找到了一个新的数据集[InferredBugs](https://github.com/microsoft/InferredBugs)，主要包含了以下信息：
 **基础信息：**
| 字段名               | 含义                                                                             |                  |                                             |
| ----------------- | ------------------------------------------------------------------------------ | ---------------- | ------------------------------------------- |
| `bug_class`       | `"PROVER"`：指这是通过形式化验证（如静态分析）工具发现的问题                                            |                  |                                             |
| `kind`            | `"ERROR"`：表示这是一个错误（不是警告或建议）                                                    |                  |                                             |
| `bug_type`        | `"RESOURCE_LEAK"`：资源泄露错误，典型如打开文件流/网络连接未关闭                                      |                  |                                             |
| `bug_type_hum`    | `"Resource Leak"`：错误的“人类可读”描述                                                  |                  |                                             |
| `qualifier`       | 错误具体描述：资源 `InputStreamReader` 在 119 行创建，但直到 138 行后都未被释放，且 121 行可能抛异常（资源未关闭的路径） |                  |                                             |
| `severity`        | `"HIGH"`：严重等级是高                                                                |                  |                                             |
| `visibility`      | `"user"`：可见性说明这个错误对用户是可见或可能产生影响                                                |                  |                                             |
| `key`             | \`"FileManager.java                                                            | getConfigContent | RESOURCE\_LEAK"\`：标识此错误的唯一键（由文件名+方法+错误类型构成） |
| `node_key`        | 一种 hash 表示错误位置                                                                 |                  |                                             |
| `hash`            | 错误的 hash 值，用于唯一标识该问题                                                           |                  |                                             |
| `censored_reason` | 为空，说明没有敏感信息被省略                                                                 |                  |                                             |

**出错位置与上下文信息：**

| 字段名                    | 含义                                                              |
| ---------------------- | --------------------------------------------------------------- |
| `line`                 | 138：表示出错的具体代码行数                                                 |
| `column`               | -1：表示列信息不可用或不适用                                                 |
| `file`                 | `src/main/java/com/hm/achievement/utils/FileManager.java`：源文件路径 |
| `procedure`            | `Reader FileManager.getConfigContent(File)`：出错的函数签名             |
| `procedure_id`         | 该函数的全限定签名，带上了返回类型和唯一 hash                                       |
| `procedure_start_line` | 109：该方法的开始行号                                                    |

很可惜的是这里面没有关于test的信息，所以暂时先搁置一边。

而后又找了[mineSStuBs](https://github.com/mast-group/mineSStuBs)但是这玩意是个静态分析工具...然后还有[Introjavaclass](https://github.com/Spirals-Team/IntroClassJava)也是太久远了，2015年之前的东西了。

---
5.7-5.19
元铭宇：
1. 已经写好了和ds交互的脚本，并且成功对每一个prompt生成了补丁
2. 对于生成的补丁，写好了脚本可以实现
   1. 从prompt的error code部分提取错误的源码
   2. 在defects4j中找到对应文件中的源码
   3. 将源码替换为ds生成的patch中的错误
   4. 用defects4j测试，记录结果到log中
   5. 将源码还原
3. 结果：25,20,19,22,18,20,16
  + 最开始                                                                                                   25
  + 人类：1. Failed test → Error → Test line → Error Code Block → Buggy code                                 20          
  + 从 code 启发（Bottom-up）：2. Buggy code → Error Code Block → Test line → Failed test → Error              19
  + Test-first 策略：3. Error Code Block → Test line → Failed test → Error → Buggy code                      22
  + Buggy-code 优先（模仿 CodeT5、Codex）：4. Buggy code → Test line → Failed test → Error Code Block → Error  18
  + 先报错，后源码（反向推理）：5. Error → Test line → Failed test → Error Code Block → Buggy code               20
  +  LLM 默认 style：6. Buggy code → Failed test → Error → Error Code Block → Test line                       16

后续结合gpt来进行分析趴
上传一下与ds交互的代码以及大家应该用不到的替换patch到源码的脚本

**解释一下如何使用和LLM交互的脚本**：[communicate.py](元铭宇/my_code/debug_with_gpt/communicate.py)
运行前请设置ds的api（因为git push不允许上传密钥这样的东西，所以就不写在这了）
一共四个参数
+ --filepath：要读取的文件路径
+ --folder：文件夹路径，会读取文件夹中所有文件
+ --prefix：提问前缀，例如：请分析以下代码：
+ --model：模型名称（默认 deepseek-chat）

正常情况下只需要填写filepath即可，如命令
```bash
python3 communicate.py --filepath your_file_path
```
该文件夹应该只含有prompt.md文件
需要注意的是在脚本的第43行，如果你对prompt文件的命名是自己设置的，需要在这里进行修改（43行的意思是将.md后缀改为_reply.md后缀作为LLM回复的答案，具体回复的格式可以查看[样例Lang_1_reply.md](元铭宇/my_code/generate_prompt/bug_info/Lang_1_reply.md)）**这点请自行更改**。最后，LLM的回复内容自动保存到folder目录下，**如果需要放到其他文件夹请自行设置**
